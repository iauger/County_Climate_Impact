{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "268141d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iauge\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\auth\\_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# Set up client\n",
    "client = bigquery.Client(project=\"drexel-msds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551c3464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target years and columns\n",
    "years = range(2010, 2021)\n",
    "\n",
    "columns = [\n",
    "    \"geo_id\", \"median_income\", \"total_pop\", \"median_age\",\n",
    "    \"white_pop\", \"black_pop\", \"hispanic_pop\", \"asian_pop\",\n",
    "    \"bachelors_degree_or_higher_25_64\",\n",
    "    \"less_than_high_school_graduate\",\n",
    "    \"some_college_and_associates_degree\",\n",
    "    \"different_house_year_ago_same_city\",\n",
    "    \"different_house_year_ago_different_city\",\n",
    "    \"median_rent\",\n",
    "    \"percent_income_spent_on_rent\",\n",
    "    \"rent_over_50_percent\",\n",
    "    \"poverty\", \"gini_index\"\n",
    "]\n",
    "\n",
    "# Dynamically generate SQL with UNION ALL\n",
    "selects = []\n",
    "for year in years:\n",
    "    selects.append(f\"\"\"\n",
    "    SELECT \n",
    "        {', '.join(columns)},\n",
    "        {year} AS year\n",
    "    FROM `bigquery-public-data.census_bureau_acs.county_{year}_5yr`\n",
    "    \"\"\")\n",
    "\n",
    "query = \"\\nUNION ALL\\n\".join(selects)\n",
    "\n",
    "# Run the query\n",
    "df = client.query(query).to_dataframe()\n",
    "\n",
    "# Force geo_id to string with zero-padding if needed\n",
    "df[\"geo_id\"] = df[\"geo_id\"].astype(str).str.zfill(5)\n",
    "\n",
    "# Extract state and county FIPS codes\n",
    "df[\"state_fips\"] = df[\"geo_id\"].str[:2]\n",
    "df[\"county_fips\"] = df[\"geo_id\"].str[2:]\n",
    "\n",
    "# Reorder columns \n",
    "cols = [\"geo_id\", \"state_fips\", \"county_fips\", \"year\"] + [\n",
    "    col for col in df.columns if col not in [\"geo_id\", \"state_fips\", \"county_fips\", \"year\"]\n",
    "]\n",
    "df = df[cols]\n",
    "\n",
    "# Save locally\n",
    "df.to_csv(\"acs_multi_year_panel.csv\", index=False)\n",
    "\n",
    "print(\"Data downloaded and saved as 'acs_multi_year_panel.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c95d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target years\n",
    "years = list(range(2000, 2025))\n",
    "\n",
    "# Core metrics: mean temp, max temp, precipitation\n",
    "base_columns = [\n",
    "    \"stn\", \"wban\", \"year\", \"mo\", \"da\", \"temp\", \"max\", \"prcp\",\n",
    "    \"hail\", \"thunder\", \"tornado_funnel_cloud\"\n",
    "]\n",
    "\n",
    "# Prepare query for each year\n",
    "# Where statement addresses null values\n",
    "queries = []\n",
    "for year in years:\n",
    "    table = f\"`bigquery-public-data.noaa_gsod.gsod{year}`\"\n",
    "    queries.append(f\"\"\"\n",
    "        SELECT \n",
    "            {', '.join(base_columns)}\n",
    "        FROM {table}\n",
    "        WHERE temp < 9999.9\n",
    "          AND prcp < 99.99\n",
    "          AND max < 9999.9\n",
    "    \"\"\")\n",
    "\n",
    "# Combine into one big UNION ALL query\n",
    "union_query = \"\\nUNION ALL\\n\".join(queries)\n",
    "\n",
    "# Run and load into DataFrame\n",
    "df = client.query(union_query).to_dataframe()\n",
    "\n",
    "# Create full date field\n",
    "df[\"date\"] = pd.to_datetime(df[[\"year\", \"mo\", \"da\"]].astype(str).agg(\"-\".join, axis=1), errors=\"coerce\")\n",
    "\n",
    "# Preview + save\n",
    "print(df.head())\n",
    "df.to_csv(\"gsod_daily_2000_2024.csv\", index=False)\n",
    "\n",
    "print(\"GSOD daily data downloaded and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b89ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gsod_by_year(filepath, output_file, years=range(2000, 2025), chunksize=1_000_000):\n",
    "    gsod_dtypes = {\n",
    "        \"stn\": str,\n",
    "        \"wban\": str,\n",
    "        \"year\": int,\n",
    "        \"mo\": int,\n",
    "        \"da\": int,\n",
    "        \"temp\": float,\n",
    "        \"max\": float,\n",
    "        \"prcp\": float,\n",
    "        \"hail\": str,\n",
    "        \"thunder\": str,\n",
    "        \"tornado_funnel_cloud\": str\n",
    "    }\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for year in years:\n",
    "        print(f\"Processing year: {year}\")\n",
    "        year_chunks = []\n",
    "\n",
    "        chunk_iter = pd.read_csv(\n",
    "            filepath,\n",
    "            dtype=gsod_dtypes,\n",
    "            parse_dates=[\"date\"],\n",
    "            chunksize=chunksize,\n",
    "            low_memory=False\n",
    "        )\n",
    "\n",
    "        # Collect chunks for the given year\n",
    "        for chunk in chunk_iter:\n",
    "            chunk = chunk[chunk[\"year\"] == year]\n",
    "            if not chunk.empty:\n",
    "                year_chunks.append(chunk)\n",
    "\n",
    "        if not year_chunks:\n",
    "            print(f\"No data for {year}\")\n",
    "            continue\n",
    "\n",
    "        year_df = pd.concat(year_chunks, ignore_index=True)\n",
    "\n",
    "        # Add derived fields\n",
    "        year_df[\"quarter\"] = year_df[\"date\"].dt.quarter\n",
    "        year_df[\"heat_day_90f\"] = year_df[\"max\"] > 90.0\n",
    "        year_df[\"stn\"] = year_df[\"stn\"].astype(str).str.zfill(6)\n",
    "        year_df[\"wban\"] = year_df[\"wban\"].astype(str).str.zfill(5)\n",
    "        year_df[\"station_id\"] = year_df[\"stn\"] + \"-\" + year_df[\"wban\"]\n",
    "\n",
    "        # Convert flags to binary\n",
    "        for col in [\"hail\", \"thunder\", \"tornado_funnel_cloud\"]:\n",
    "            year_df[col] = pd.to_numeric(year_df[col], errors=\"coerce\").fillna(0).astype(\"int8\")\n",
    "\n",
    "        # Group and aggregate\n",
    "        grouped = year_df.groupby([\"station_id\", \"year\", \"quarter\"]).agg(\n",
    "            avg_temp=(\"temp\", \"mean\"),\n",
    "            max_temp=(\"max\", \"max\"),\n",
    "            total_precip=(\"prcp\", \"sum\"),\n",
    "            heat_days_90F=(\"heat_day_90f\", \"sum\"),\n",
    "            hail_days=(\"hail\", \"sum\"),\n",
    "            thunder_days=(\"thunder\", \"sum\"),\n",
    "            tornado_days=(\"tornado_funnel_cloud\", \"sum\"),\n",
    "            num_days=(\"date\", \"count\")\n",
    "        ).reset_index()\n",
    "\n",
    "        all_results.append(grouped)\n",
    "        print(f\"Year {year} processed: {len(grouped)} station-quarter records.\")\n",
    "\n",
    "    # Final output\n",
    "    full_df = pd.concat(all_results, ignore_index=True)\n",
    "    full_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nAll years saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce3d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_gsod_by_year(\n",
    "    filepath=\"gsod_daily_2000_2024.csv\",\n",
    "    output_file=\"gsod_quarterly_aggregates.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c159266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying storm events for 2000...\n",
      "Querying storm events for 2001...\n",
      "Querying storm events for 2002...\n",
      "Querying storm events for 2003...\n",
      "Querying storm events for 2004...\n",
      "Querying storm events for 2005...\n",
      "Querying storm events for 2006...\n",
      "Querying storm events for 2007...\n",
      "Querying storm events for 2008...\n",
      "Querying storm events for 2009...\n",
      "Querying storm events for 2010...\n",
      "Querying storm events for 2011...\n",
      "Querying storm events for 2012...\n",
      "Querying storm events for 2013...\n",
      "Querying storm events for 2014...\n",
      "Querying storm events for 2015...\n",
      "Querying storm events for 2016...\n",
      "Querying storm events for 2017...\n",
      "Querying storm events for 2018...\n",
      "Querying storm events for 2019...\n",
      "Querying storm events for 2020...\n",
      "Querying storm events for 2021...\n",
      "Querying storm events for 2022...\n",
      "Querying storm events for 2023...\n",
      "Querying storm events for 2024...\n",
      "All storm event data loaded: (215730, 8)\n"
     ]
    }
   ],
   "source": [
    "years = range(2000, 2025) \n",
    "all_dfs = []\n",
    "\n",
    "for year in years:\n",
    "    print(f\"Querying storm events for {year}...\")\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "      CONCAT(LPAD(state_fips_code, 2, '0'), LPAD(cz_fips_code, 3, '0')) AS geo_id,\n",
    "      EXTRACT(YEAR FROM event_begin_time) AS year,\n",
    "      event_type,\n",
    "      COUNT(*) AS num_events,\n",
    "      SUM(damage_property) AS total_property_damage,\n",
    "      SUM(damage_crops) AS total_crop_damage,\n",
    "      SUM(injuries_direct + injuries_indirect) AS total_injuries,\n",
    "      SUM(deaths_direct + deaths_indirect) AS total_deaths\n",
    "    FROM\n",
    "      `bigquery-public-data.noaa_historic_severe_storms.storms_{year}`\n",
    "    WHERE\n",
    "      cz_type = 'C'\n",
    "    GROUP BY\n",
    "      geo_id, year, event_type\n",
    "    \"\"\"\n",
    "\n",
    "    df = client.query(query).to_dataframe()\n",
    "    all_dfs.append(df)\n",
    "\n",
    "# Combine all years\n",
    "storm_df = pd.concat(all_dfs, ignore_index=True)\n",
    "print(\"All storm event data loaded:\", storm_df.shape)\n",
    "\n",
    "# Optional: Save to CSV\n",
    "storm_df.to_csv(\"storm_events_by_county_year.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2b6578",
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_wide = pivot_storm_events_by_type(storm_df)\n",
    "print(storm_wide.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2955be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = gpd.read_file(r\"cb_2023_us_county_5m\\cb_2023_us_county_5m.shp\")\n",
    "\n",
    "# Project to WGS84 (matches lat/lon)\n",
    "counties = counties.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Create a unique FIPS code for join\n",
    "counties[\"geo_id\"] = counties[\"STATEFP\"] + counties[\"COUNTYFP\"]\n",
    "\n",
    "# Select relevant columns\n",
    "counties = counties[[\"geo_id\", \"NAME\", \"STATEFP\", \"COUNTYFP\", \"geometry\"]]\n",
    "counties.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694a6c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "  usaf,\n",
    "  wban,\n",
    "  name,\n",
    "  country,\n",
    "  state,\n",
    "  lat,\n",
    "  lon,\n",
    "  elev,\n",
    "  `begin`,\n",
    "  `end`\n",
    "FROM \n",
    "  `bigquery-public-data.noaa_gsod.stations`\n",
    "\"\"\"\n",
    "\n",
    "stations_df = client.query(query).to_dataframe()\n",
    "\n",
    "stations_df[\"usaf\"] = stations_df[\"usaf\"].str.zfill(6)\n",
    "stations_df[\"wban\"] = stations_df[\"wban\"].str.zfill(5)\n",
    "stations_df[\"station_id\"] = stations_df[\"usaf\"] + \"-\" + stations_df[\"wban\"]\n",
    "stations_df[\"begin\"] = pd.to_datetime(stations_df[\"begin\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "stations_df[\"end\"] = pd.to_datetime(stations_df[\"end\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "\n",
    "stations_df = stations_df[stations_df[\"country\"].str.upper() == \"US\"]\n",
    "\n",
    "# Filter for stations active at any point during analysis window\n",
    "mask = (stations_df[\"begin\"] <= \"2024-12-31\") & (stations_df[\"end\"] >= \"2000-01-01\")\n",
    "stations_df = stations_df[mask]\n",
    "\n",
    "stations_df.to_csv(\"noaa_stations.csv\", index=False)\n",
    "stations_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
